# Moss Audio Tokenizer ONNX

![MIT License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)
![ONNX](https://img.shields.io/badge/ONNX-1.20.1-green.svg)
![Onnx Runtime](https://img.shields.io/badge/Onnx%20Runtime-1.23.2-blue.svg)
![NumPy](https://img.shields.io/badge/NumPy-1.26.4-blue.svg)

## Model Introduction

MOSSAudioTokenizer is a unified discrete audio tokenizer based on the Cat (Causal Audio Tokenizer with Transformer) architecture. The model has 1.6 billion parameters and can serve as a unified discrete interface that simultaneously achieves lossless quality reconstruction and high-level semantic alignment.

Main features:

    Extreme compression and variable bit rate: Compresses 24kHz raw audio to an extremely low frame rate of 12.5Hz. Through 32-layer residual vector quantization (RVQ), it supports high-fidelity reconstruction across a wide range of bit rates from 0.125kbps to 4kbps.
    Pure Transformer architecture: The model adopts a "CNN-free" homogeneous architecture, completely built from causal Transformer modules. The encoder and decoder combined have 1.6 billion parameters, ensuring excellent scalability and supporting low-latency streaming inference.
    Large-scale general audio training: Trained on 3 million hours of diverse audio data, it can excellently encode and reconstruct content from all audio domains, including speech, sound effects, and music.
    Unified semantic-acoustic representation: While achieving state-of-the-art reconstruction quality, the discrete tokens generated by Cat have "rich semantics," making them ideal for downstream tasks such as speech understanding (ASR) and speech synthesis (TTS).
    Fully trained from scratch: Cat does not depend on any pre-trained encoders (such as HuBERT or Whisper) nor does it use teacher model distillation. All representations are learned autonomously from raw data.
    End-to-end joint optimization: All components—including encoder, quantizer, decoder, discriminator, and decoder-only LLM for semantic alignment—are jointly optimized in a single unified training process.

Summary: By combining a concise and scalable architecture with large-scale data, the Cat architecture breaks through the bottlenecks of traditional audio tokenizers, providing a robust, high-fidelity, and semantically solid interface for next-generation native audio foundation models.

This repository contains the ONNX model files for MOSSAudioTokenizer Decoder, as well as Python code for using the model for audio decoding.

![Architecture Diagram](./rep_sources/arch.png)

## Open Source Repositories

 - ModelScope: [https://www.modelscope.cn/models/KeanuX/MossAudioTokenizer-ONNX](https://www.modelscope.cn/models/KeanuX/MossAudioTokenizer-ONNX)

 - GitHub: [https://github.com/201831771214/MossAudioTokenizer-ONNX.git](https://github.com/201831771214/MossAudioTokenizer-ONNX.git)

## Clone Repository

```shell
# Get repository source code
git clone https://github.com/201831771214/MossAudioTokenizer-ONNX.git

# Download model
modelscope download --model KeanuX/MossAudioTokenizer-ONNX --local_dir ./
```

### Git Repository Structure

```shell
./
├── audio_tokens.npy
├── check_onnx.py
├── export_audio_tokenizer.py
├── export_model_info.py
├── generated_audio.wav
├── infos
│   └── audio_tokenizer_decoder.info
├── logs
│   ├── check_onnx.log
│   └── run_onnx.log
├── models
│   └── moss_audio_tokenizer_onnx
│       ├── 2005f62a-1458-11f1-80d4-cc28aa3bf0f5.data
│       ├── 2799b6f6-1458-11f1-80d4-cc28aa3bf0f5.data
│       ├── audio_tokenizer_decoder.onnx
│       └── audio_tokenizer_encoder.onnx
├── moss_audio_tokenizer
│   ├── config.json
│   ├── configuration_moss_audio_tokenizer.py
│   ├── demo
│   │   ├── demo_gt.wav
│   │   └── test_reconstruction.py
│   ├── images
│   │   ├── arch.png
│   │   ├── metrics_on_librispeech_test_clean.png
│   │   ├── mosi-logo.png
│   │   ├── OpenMOSS_Logo.png
│   │   └── reconstruct_comparison_table.png
│   ├── __init__.py
│   ├── LICENSE
│   ├── modeling_moss_audio_tokenizer.py
│   ├── __pycache__
│   │   ├── configuration_moss_audio_tokenizer.cpython-310.pyc
│   │   ├── __init__.cpython-310.pyc
│   │   └── modeling_moss_audio_tokenizer.cpython-310.pyc
│   ├── README.md
│   └── requirements.txt
├── moss_extra
│   ├── audio_tokenizer_decoder.py
│   └── audio_tokenizer_encoder.py
├── README_Git.md
├── README_ModelScope.md
├── rep_sources
│   ├── arch.png
│   ├── License-MIT-yellow.png
│   ├── NumPy-1.26.4-blue.png
│   ├── ONNX-1.20.1-green.png
│   ├── Onnx Runtime-1.23.2-blue.png
│   └── Python-3.10+-blue.png
├── requirements.txt
├── run_decoder.py
└── run_encoder.py

11 directories, 42 files
```

## Usage

### Model Information

```txt
#### Audio Decoder Infos ####
============================================================
Basic ONNX Model Information
============================================================
Model file path: ./models/moss_audio_tokenizer_decoder_onnx/audio_tokenizer_decoder.onnx
ONNX version: 7
Producer information: pytorch 2.8.0
Model version: 0
Description: 

============================================================
Model Input Information (Total 1 input)
============================================================
Input 1: audio_codes
  Data type: int32
  Shape: [0, 0, 0]

============================================================
Model Output Information (Total 1 output)
============================================================
Output 1: audio
  Data type: float32
  Shape: [0, 0]

#### Audio Encoder Infos ####
============================================================
Basic ONNX Model Information
============================================================
Model file path: ./models/moss_audio_tokenizer_decoder_onnx/audio_tokenizer_encoder.onnx
ONNX version: 7
Producer information: pytorch 2.8.0
Model version: 0
Description: 

============================================================
Model Input Information (Total 1 input)
============================================================
Input 1: input_values
  Data type: float32
  Shape: [0, 0, 0]

============================================================
Model Output Information (Total 3 outputs)
============================================================
Output 1: audio_codes
  Data type: int64
  Shape: [32, 0, 0]

Output 2: audio_codes_lengths
  Data type: int64
  Shape: [0]

Output 3: encoder_hidden_states
  Data type: float32
  Shape: [0, 768, 0]
```

For detailed information, please refer to: [ONNX Model Information](https://github.com/201831771214/MossAudioTokenizer-ONNX/infos/)

### Install Dependencies

```shell
# Install dependencies
pip install -r requirements.txt
```

### Quick Start

#### Run Audio Tokenizer Decoder

```python
import onnxruntime as ort
import numpy as np
from numpy.typing import NDArray
import soundfile as sf
from typing import Tuple
import os
import sys

import logging

logger = logging.getLogger(__name__)
file_handler = logging.FileHandler("./logs/run_decoder.log", mode="w", encoding="utf-8")
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
file_handler.setFormatter(formatter)
file_handler.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.setLevel(logging.INFO)

class AudioTokenizerDecoder:
    def __init__(self, model_path:str, device:str="cuda"):
        self.model_path = model_path
        self.device = device.lower()
        
        logger.info(f"All available providers: {ort.get_available_providers()}")
        if device == "cuda" and "CUDAExecutionProvider" in ort.get_available_providers():
            self.providers = ["CUDAExecutionProvider"]
        elif device == "cpu" and "CPUExecutionProvider" in ort.get_available_providers():
            self.providers = ["CPUExecutionProvider"]
        else:
            logger.warning(f"Device {device} is not supported. Fall back to CPU.")
            self.providers = ["CPUExecutionProvider"]
        
        # Configure session options for memory optimization
        sess_options = ort.SessionOptions()
        sess_options.enable_mem_pattern = False
        sess_options.enable_mem_reuse = False
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
        
        self.session = ort.InferenceSession(model_path, providers=self.providers, sess_options=sess_options)
        
        self.input_names = [input.name for input in self.session.get_inputs()]
        self.output_names = [output.name for output in self.session.get_outputs()]
        logger.info(f"Session Input names: {self.input_names}")
        logger.info(f"Session Output names: {self.output_names}")
        
    def __decode(self, audio_tokens:np.ndarray) -> Tuple[np.ndarray, int]:
        """Decode audio tokens into audio waveform.

        Args:
            audio_tokens (np.ndarray): Audio tokens of shape (batch_size, channels, audio_tokens_len).

        Returns:
            audio (np.ndarray): Audio waveform of shape (batch_size, audio_len).
            sample_rate (int): Sample rate of the audio waveform.
            
        """
        input_spec = {
            self.input_names[0]: audio_tokens
        }
        
        audio = self.session.run(self.output_names, input_spec)[0]
        return audio, 24000

    # TODO: Both support dynamic input shape and static input shape, when the input shape is static, the chunk_size should be the same as the audio_tokens_len.
    def decode_chunked(self, audio_tokens:np.ndarray, chunk_size:int=200) -> Tuple[np.ndarray, int]:
        """Decode audio tokens in chunks to manage memory."""
        batch_size, channels, seq_len = audio_tokens.shape
        
        if seq_len <= chunk_size:
            return self.__decode(audio_tokens)
        
        # Process in chunks
        chunks = []
        for i in range(0, seq_len, chunk_size):
            chunk = audio_tokens[:, :, i:i+chunk_size]
            chunk_audio, _ = self.__decode(chunk)
            chunks.append(chunk_audio)
        
        # Concatenate the results (this might need adjustment based on model behavior)
        full_audio = np.concatenate(chunks, axis=-1)  # This depends on the output shape
        return full_audio, 24000
    
test_audio_tokens = "./audio_tokens.npy"
model_path = "./models/moss_audio_tokenizer_decoder_onnx/audio_tokenizer_decoder.onnx"
output_path = "generated_audio.wav"

if __name__ == "__main__":
    audio_decoder = AudioTokenizerDecoder(model_path)
    audio_tokens:NDArray = np.load(test_audio_tokens)
    if audio_tokens.ndim != 3:
        audio_tokens = np.expand_dims(audio_tokens, axis=0).astype(np.int32)
    logger.info(f"Audio tokens shape: {audio_tokens.shape}")
    
    audio, sample_rate = audio_decoder.decode_chunked(audio_tokens)
    logger.info(f"Generated Audio shape: {audio.shape}")
    
    if audio.ndim == 2:
        audio = audio[0]
    
    sf.write(output_path, audio, sample_rate)
    logger.info(f"Audio waveform saved to {output_path} with sample rate {sample_rate}")
```

##### Decode Audio Tokens Result

 - Generated audio file: [generated_audio.wav](https://github.com/201831771214/MossAudioTokenizer-ONNX/blob/main/generated_audio.wav)

 - Audio Text: "MOSS-TTS-Realtime is a context-aware, multi-turn streaming TTS model designed specifically for real-time voice agents. By combining text dialogue history and users' previous acoustic characteristics, it provides low-latency, coherent and consistent voice responses in multi-turn interactions."

#### Run Audio Tokenizer Encoder

```python
import onnxruntime as ort
import numpy as np
from numpy.typing import NDArray
import librosa
from typing import Tuple
import os
import sys

import logging

logger = logging.getLogger(__name__)
file_handler = logging.FileHandler("./logs/run_encoder.log", mode="w", encoding="utf-8")
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
file_handler.setFormatter(formatter)
file_handler.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.setLevel(logging.INFO)

class AudioTokenizerEncoder:
    def __init__(self, model_path:str, device:str="cuda"):
        self.model_path = model_path
        self.device = device.lower()
        
        logger.info(f"All available providers: {ort.get_available_providers()}")
        if device == "cuda" and "CUDAExecutionProvider" in ort.get_available_providers():
            self.providers = ["CUDAExecutionProvider"]
        elif device == "cpu" and "CPUExecutionProvider" in ort.get_available_providers():
            self.providers = ["CPUExecutionProvider"]
        else:
            logger.warning(f"Device {device} is not supported. Fall back to CPU.")
            self.providers = ["CPUExecutionProvider"]
        
        # Configure session options for memory optimization
        sess_options = ort.SessionOptions()
        sess_options.enable_mem_pattern = False
        sess_options.enable_mem_reuse = False
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
        
        self.session = ort.InferenceSession(model_path, providers=self.providers, sess_options=sess_options)
        
        self.input_names = [input.name for input in self.session.get_inputs()]
        self.output_names = [output.name for output in self.session.get_outputs()]
        logger.info(f"Session Input names: {self.input_names}")
        logger.info(f"Session Output names: {self.output_names}")
        
    def encode(self, audio_data:np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Encode audio waveform into audio tokens.

        Args:
            audio_data (np.ndarray): Audio waveform of shape (batch_size, channels, audio_len).

        Returns:
            audio_codes (np.ndarray): Audio tokens of shape (num_layers, batch_size, seq_len).
            audio_codes_lengths (np.ndarray): Audio tokens lengths of shape (seq_len,).
            encoder_hidden_states (np.ndarray): Encoder hidden states of shape (batch_size, d_model, seq_len).
            
        """
        input_spec = {
            self.input_names[0]: audio_data
        }
        
        audio_codes, audio_codes_lengths, encoder_hidden_states = self.session.run(self.output_names, input_spec)
        
        
        return audio_codes, audio_codes_lengths, encoder_hidden_states

test_audio = "./sources/audios/dubowen.wav"
model_path = "./models/moss_audio_tokenizer_decoder_onnx/audio_tokenizer_encoder.onnx"

if __name__ == "__main__":
    audio_encoder = AudioTokenizerEncoder(model_path)
    
    audio_data, sample_rate = librosa.load(test_audio, sr=24000, dtype=np.float32)
    logger.info(f"Audio data shape: {audio_data.shape}")
    
    # Add batch size and channel dimension if not present
    if audio_data.ndim == 1:
        audio_data = np.expand_dims(audio_data, axis=0)
        audio_data = np.expand_dims(audio_data, axis=1)
    
    audio_codes, audio_codes_lengths, encoder_hidden_states = audio_encoder.encode(audio_data)
    logger.info(f"Audio codes shape: {audio_codes.shape} ---- {audio_codes}")
    logger.info(f"Audio codes lengths shape: {audio_codes_lengths.shape} ---- {audio_codes_lengths}")
    logger.info(f"Encoder hidden states shape: {encoder_hidden_states.shape} ---- {encoder_hidden_states}")
```

##### Audio Tokenizer Encoder Result

```shell
2026-02-28 14:30:43,491 - INFO - Audio codes shape: (32, 1, 69) ---- [[[ 817   57  132 ...  335  335  950]]

 [[ 470   18  632 ...  175  667  370]]

 [[ 224  363  968 ...  516  282  126]]

 ...

 [[ 125  958  275 ...  794  610  522]]

 [[ 963  217  460 ...  468  575  452]]

 [[1014  367  950 ...  968  688  369]]]
2026-02-28 14:30:43,491 - INFO - Audio codes lengths shape: (1,) ---- [68]
2026-02-28 14:30:43,492 - INFO - Encoder hidden states shape: (1, 768, 69) ---- [[[ 9.686964   -5.875668   -4.5082426  ...  4.122473    4.668351
    4.7193575 ]
  [-3.454163   -1.0035998   2.433176   ... -0.72330576 -1.1036214
   -1.0855117 ]
  [-2.9715493   2.7055      2.9065142  ... -1.2198896   3.042768
    2.9706757 ]
  ...
  [-6.3878407  -8.618917    8.772977   ...  4.81421     4.5244756
    4.5736403 ]
  [-4.635868    9.056893    6.7735367  ... -3.4137735  -4.1788063
   -4.2336264 ]
  [ 4.5891705  12.405132   -1.1379069  ... -2.1964636  -2.6125443
   -2.6521518 ]]]
```

### Join Us

 - WeChat Official Account: "CrazyNET"

### Follow the CrazyNET WeChat Official Account to get more information and updates about MOSS-TTS-Realtime and MossAudioTokenizer.

